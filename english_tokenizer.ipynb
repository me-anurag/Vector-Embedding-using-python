{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  English Tokenizer & Vector Embeddings\n",
    "### GPT-2 BPE · Word2Vec CBOW Training · Attention Mechanism · Embedding Arithmetic\n",
    "---\n",
    "**Topics Covered:**\n",
    "1. Simple Regex Tokenizer V1 & V2 (from scratch)\n",
    "2. GPT-2 BPE Tokenizer via tiktoken\n",
    "3. Sliding Window Dataset & DataLoader\n",
    "4. Token + Positional Embeddings\n",
    "5. Word2Vec CBOW Training\n",
    "6. Training Loss Curve\n",
    "7. Cosine Similarity — many word pairs\n",
    "8. Embedding Arithmetic (king − man + woman)\n",
    "9. PCA 2D Visualization\n",
    "10. t-SNE Visualization\n",
    "11. Cosine Similarity Heatmap\n",
    "12. Self-Attention from Scratch\n",
    "13. Masked (Causal) Attention — GPT style\n",
    "14. Random vs Trained Comparison\n",
    "15. Full Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 1 — Install & Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install torch tiktoken requests matplotlib scikit-learn\n",
    "\n",
    "import sys\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"Python   :\", sys.version)\n",
    "print(\"torch    :\", version(\"torch\"))\n",
    "print(\"tiktoken :\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 2 — Download & Load English Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, requests\n",
    "\n",
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/the-verdict.txt\")\n",
    "    r = requests.get(url, timeout=30)\n",
    "    r.raise_for_status()\n",
    "    with open(\"the-verdict.txt\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    print(\"Downloaded the-verdict.txt\")\n",
    "\n",
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(f\"Total characters : {len(raw_text):,}\")\n",
    "print(f\"Total words      : {len(raw_text.split()):,}\")\n",
    "print(f\"\\nFirst 200 chars:\\n{raw_text[:200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 3 — Regex Tokenizer (From Scratch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [t.strip() for t in preprocessed if t.strip()]\n",
    "print(f\"Total tokens (with dupes): {len(preprocessed):,}\")\n",
    "print(f\"Sample: {preprocessed[:15]}\")\n",
    "\n",
    "all_words = sorted(set(preprocessed))\n",
    "all_words += [\"<|endoftext|>\", \"<|unk|>\"]\n",
    "vocab = {token: i for i, token in enumerate(all_words)}\n",
    "\n",
    "print(f\"\\nVocabulary size: {len(vocab):,}\")\n",
    "print(\"\\nSample vocab entries:\")\n",
    "for tok, idx in list(vocab.items())[:10]:\n",
    "    print(f\"  '{tok}' -> {idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 4 — SimpleTokenizerV1 & V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    \"\"\"Raises error on unknown tokens\"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [t.strip() for t in tokens if t.strip()]\n",
    "        return [self.str_to_int[t] for t in tokens]\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "class SimpleTokenizerV2:\n",
    "    \"\"\"Handles unknown tokens with <|unk|>\"\"\"\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i: s for s, i in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.:;?_!\"()\\']|--|\\s)', text)\n",
    "        tokens = [t.strip() for t in tokens if t.strip()]\n",
    "        tokens = [t if t in self.str_to_int else \"<|unk|>\" for t in tokens]\n",
    "        return [self.str_to_int[t] for t in tokens]\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        return re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "\n",
    "tok_v1 = SimpleTokenizerV1(vocab)\n",
    "tok_v2 = SimpleTokenizerV2(vocab)\n",
    "\n",
    "s1 = \"I HAD always thought Jack Gisburn rather a cheap genius.\"\n",
    "ids = tok_v1.encode(s1)\n",
    "print(\"=== V1 ===\")\n",
    "print(f\"Input  : {s1}\")\n",
    "print(f\"IDs    : {ids}\")\n",
    "print(f\"Decoded: {tok_v1.decode(ids)}\")\n",
    "\n",
    "s2 = \"Hello there! zxqfoo is unknown.\"\n",
    "ids2 = tok_v2.encode(s2)\n",
    "print(\"\\n=== V2 (unknown token) ===\")\n",
    "print(f\"Input  : {s2}\")\n",
    "print(f\"IDs    : {ids2}\")\n",
    "print(f\"Decoded: {tok_v2.decode(ids2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 5 — GPT-2 BPE Tokenizer via tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "print(f\"GPT-2 Vocab Size: {tokenizer.n_vocab:,}\")\n",
    "print(f\"Total tokens in dataset: {len(tokenizer.encode(raw_text)):,}\")\n",
    "\n",
    "sample = \"Hello, I am learning about tokenizers and word embeddings!\"\n",
    "enc    = tokenizer.encode(sample)\n",
    "print(f\"\\nInput   : {sample}\")\n",
    "print(f\"IDs     : {enc}\")\n",
    "print(f\"Decoded : {tokenizer.decode(enc)}\")\n",
    "\n",
    "print(\"\\n--- Token-by-Token Breakdown ---\")\n",
    "for tid in enc:\n",
    "    print(f\"  ID {tid:6d}  ->  '{tokenizer.decode([tid])}'\")\n",
    "\n",
    "eot = tokenizer.encode(\"<|endoftext|>\", allowed_special={\"<|endoftext|>\"})\n",
    "print(f\"\\n<|endoftext|> ID: {eot}\")\n",
    "\n",
    "combined = \"Hello! <|endoftext|> New document starts.\"\n",
    "print(f\"Combined IDs: {tokenizer.encode(combined, allowed_special={'<|endoftext|>'})}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 6 — GPT Dataset & DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids, self.target_ids = [], []\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "        print(f\"Total tokens: {len(token_ids):,}\")\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            self.input_ids.append(torch.tensor(token_ids[i:i+max_length]))\n",
    "            self.target_ids.append(torch.tensor(token_ids[i+1:i+max_length+1]))\n",
    "    def __len__(self): return len(self.input_ids)\n",
    "    def __getitem__(self, idx): return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True):\n",
    "    tok = tiktoken.get_encoding(\"gpt2\")\n",
    "    ds  = GPTDatasetV1(txt, tok, max_length, stride)\n",
    "    return DataLoader(ds, batch_size=batch_size, shuffle=shuffle, drop_last=True)\n",
    "\n",
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=max_length,\n",
    "                                   stride=max_length, shuffle=False)\n",
    "inputs, targets = next(iter(dataloader))\n",
    "\n",
    "tok = tiktoken.get_encoding(\"gpt2\")\n",
    "print(\"\\nInput shape:\", inputs.shape)\n",
    "print(\"Input IDs:\\n\", inputs)\n",
    "print(\"\\nFirst sample:\")\n",
    "print(\"  Input :\", tok.decode(inputs[0].tolist()))\n",
    "print(\"  Target:\", tok.decode(targets[0].tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 7 — Token + Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "output_dim = 256\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "pos_embedding_layer   = torch.nn.Embedding(max_length, output_dim)\n",
    "\n",
    "token_embeddings = token_embedding_layer(inputs)\n",
    "pos_embeddings   = pos_embedding_layer(torch.arange(max_length))\n",
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "\n",
    "print(\"=== Embedding Shapes ===\")\n",
    "print(f\"inputs           : {inputs.shape}\")\n",
    "print(f\"token_embeddings : {token_embeddings.shape}\")\n",
    "print(f\"pos_embeddings   : {pos_embeddings.shape}\")\n",
    "print(f\"input_embeddings : {input_embeddings.shape}\")\n",
    "print(f\"\\nFirst token vector (8 dims): {input_embeddings[0][0][:8].detach().numpy()}\")\n",
    "\n",
    "print(\"\\n--- Dimension vs Parameter Count ---\")\n",
    "print(f\"{'Dim':<8} {'Output Shape':<22} {'Params':>12}\")\n",
    "print(\"-\"*44)\n",
    "for d in [64, 128, 256, 512, 768]:\n",
    "    print(f\"{d:<8} {'[8,4,'+str(d)+']':<22} {vocab_size*d:>12,}\")\n",
    "print(\"\\nGPT-2 Small=768 | GPT-2 Large=1280 | GPT-3=12288\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 8 — Word2Vec CBOW Training\n",
    "> CBOW: predict center word from surrounding context words.\n",
    "> This teaches embeddings to capture semantic meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Build vocabulary from raw text\n",
    "words = raw_text.lower().split()\n",
    "words = [re.sub(r'[^a-z]', '', w) for w in words]\n",
    "words = [w for w in words if len(w) > 1]\n",
    "\n",
    "freq        = Counter(words)\n",
    "vocab_words = [w for w, c in freq.most_common(500)]\n",
    "w2i         = {w: i for i, w in enumerate(vocab_words)}\n",
    "i2w         = {i: w for w, i in w2i.items()}\n",
    "V           = len(vocab_words)\n",
    "print(f\"Training vocab size: {V}\")\n",
    "\n",
    "# Build CBOW pairs  (context -> center)\n",
    "WINDOW    = 2\n",
    "cbow_data = []\n",
    "filtered  = [w for w in words if w in w2i]\n",
    "\n",
    "for i in range(WINDOW, len(filtered) - WINDOW):\n",
    "    ctx = ([filtered[i-j] for j in range(WINDOW, 0, -1)] +\n",
    "           [filtered[i+j] for j in range(1, WINDOW+1)])\n",
    "    ctr = filtered[i]\n",
    "    if all(w in w2i for w in ctx) and ctr in w2i:\n",
    "        cbow_data.append(([w2i[w] for w in ctx], w2i[ctr]))\n",
    "\n",
    "random.shuffle(cbow_data)\n",
    "print(f\"Training samples: {len(cbow_data):,}\")\n",
    "\n",
    "# CBOW Model\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear    = nn.Linear(embed_dim, vocab_size)\n",
    "    def forward(self, ctx):\n",
    "        return self.linear(self.embedding(ctx).mean(dim=1))\n",
    "\n",
    "EMBED_DIM = 64\n",
    "EPOCHS    = 8\n",
    "BATCH     = 256\n",
    "\n",
    "model     = CBOWModel(V, EMBED_DIM)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"\\nTraining Word2Vec CBOW...\")\n",
    "loss_history = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    random.shuffle(cbow_data)\n",
    "    total_loss = 0\n",
    "    steps      = 0\n",
    "    for start in range(0, len(cbow_data), BATCH):\n",
    "        batch   = cbow_data[start:start+BATCH]\n",
    "        ctx_t   = torch.tensor([d[0] for d in batch])\n",
    "        ctr_t   = torch.tensor([d[1] for d in batch])\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(ctx_t), ctr_t)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        steps      += 1\n",
    "    avg = total_loss / steps\n",
    "    loss_history.append(avg)\n",
    "    print(f\"  Epoch {epoch+1}/{EPOCHS}  Loss: {avg:.4f}\")\n",
    "\n",
    "trained_embeddings = model.embedding.weight.detach()\n",
    "print(\"\\nTraining complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 9 — Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1, EPOCHS+1), loss_history, 'o-',\n",
    "        color='#3498db', linewidth=2.5, markersize=8)\n",
    "ax.fill_between(range(1, EPOCHS+1), loss_history, alpha=0.15, color='#3498db')\n",
    "ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax.set_ylabel(\"Cross-Entropy Loss\", fontsize=12)\n",
    "ax.set_title(\"Word2Vec CBOW Training Loss\", fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"training_loss.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 10 — Cosine Similarity (Many Examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_vec(word):\n",
    "    if word not in w2i: return None\n",
    "    return trained_embeddings[w2i[word]]\n",
    "\n",
    "def cosine_sim(w1, w2):\n",
    "    v1, v2 = get_vec(w1), get_vec(w2)\n",
    "    if v1 is None or v2 is None: return None\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n",
    "\n",
    "def top_similar(word, topn=5):\n",
    "    if word not in w2i: return []\n",
    "    vec  = trained_embeddings[w2i[word]].unsqueeze(0)\n",
    "    sims = F.cosine_similarity(vec, trained_embeddings).detach().numpy()\n",
    "    top  = np.argsort(sims)[::-1][1:topn+1]\n",
    "    return [(i2w[i], sims[i]) for i in top]\n",
    "\n",
    "# Many word pair groups\n",
    "print(\"=\" * 55)\n",
    "print(\"GROUP 1 — Synonyms / Related\")\n",
    "print(\"=\" * 55)\n",
    "for w1, w2 in [(\"good\",\"great\"),(\"old\",\"new\"),(\"work\",\"time\"),(\"see\",\"look\"),(\"know\",\"think\")]:\n",
    "    s = cosine_sim(w1, w2)\n",
    "    if s: print(f\"  {w1:<10} <-> {w2:<10}  {s:+.4f}  {'|'*int((s+1)*15)}\")\n",
    "\n",
    "print(\"\\nGROUP 2 — Opposites\")\n",
    "print(\"-\" * 55)\n",
    "for w1, w2 in [(\"good\",\"bad\"),(\"come\",\"go\"),(\"old\",\"young\"),(\"right\",\"wrong\")]:\n",
    "    s = cosine_sim(w1, w2)\n",
    "    if s: print(f\"  {w1:<10} <-> {w2:<10}  {s:+.4f}  {'|'*int((s+1)*15)}\")\n",
    "\n",
    "print(\"\\nGROUP 3 — Morphological (same root)\")\n",
    "print(\"-\" * 55)\n",
    "for w1, w2 in [(\"man\",\"men\"),(\"see\",\"saw\"),(\"go\",\"went\"),(\"make\",\"made\")]:\n",
    "    s = cosine_sim(w1, w2)\n",
    "    if s: print(f\"  {w1:<10} <-> {w2:<10}  {s:+.4f}  {'|'*int((s+1)*15)}\")\n",
    "\n",
    "print(\"\\nGROUP 4 — Unrelated\")\n",
    "print(\"-\" * 55)\n",
    "for w1, w2 in [(\"time\",\"eye\"),(\"door\",\"work\"),(\"hand\",\"old\")]:\n",
    "    s = cosine_sim(w1, w2)\n",
    "    if s: print(f\"  {w1:<10} <-> {w2:<10}  {s:+.4f}  {'|'*int((s+1)*15)}\")\n",
    "\n",
    "# Most similar words\n",
    "print(\"\\n--- Top 5 Similar Words ---\")\n",
    "for q in [\"good\", \"time\", \"man\", \"come\"]:\n",
    "    sim = top_similar(q)\n",
    "    if sim:\n",
    "        print(f\"  '{q}' -> {', '.join([f\\\"{w}({s:.2f})\\\" for w,s in sim])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 11 — Embedding Arithmetic (king − man + woman)\n",
    "> Word2Vec analogy: vector(king) − vector(man) + vector(woman) ≈ vector(queen)\n",
    "> We use words available in our small dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_arithmetic(pos_words, neg_words, topn=5):\n",
    "    result = torch.zeros(EMBED_DIM)\n",
    "    used, missing = [], []\n",
    "    for w in pos_words:\n",
    "        v = get_vec(w)\n",
    "        if v is not None: result += v;  used.append(f\"+{w}\")\n",
    "        else:             missing.append(w)\n",
    "    for w in neg_words:\n",
    "        v = get_vec(w)\n",
    "        if v is not None: result -= v;  used.append(f\"-{w}\")\n",
    "        else:             missing.append(w)\n",
    "    result = F.normalize(result.unsqueeze(0), dim=1).squeeze()\n",
    "    sims   = F.cosine_similarity(result.unsqueeze(0), trained_embeddings).detach().numpy()\n",
    "    excl   = set(pos_words + neg_words)\n",
    "    ranked = np.argsort(sims)[::-1]\n",
    "    res    = []\n",
    "    for idx in ranked:\n",
    "        w = i2w[idx]\n",
    "        if w not in excl:\n",
    "            res.append((w, sims[idx]))\n",
    "        if len(res) == topn: break\n",
    "    return used, missing, res\n",
    "\n",
    "queries = [\n",
    "    ([\"men\", \"good\"],    [\"man\"],    \"men + good - man   (gender analogy)\"),\n",
    "    ([\"good\", \"great\"],  [\"bad\"],    \"good + great - bad\"),\n",
    "    ([\"see\", \"know\"],    [\"think\"],  \"see + know - think\"),\n",
    "    ([\"work\", \"time\"],   [\"come\"],   \"work + time - come\"),\n",
    "    ([\"old\"],            [\"new\"],    \"old - new\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"EMBEDDING ARITHMETIC\")\n",
    "print(\"=\" * 60)\n",
    "for pos, neg, label in queries:\n",
    "    used, miss, res = embedding_arithmetic(pos, neg)\n",
    "    print(f\"\\n  Query  : {label}\")\n",
    "    if miss: print(f\"  Missing: {miss}\")\n",
    "    print(f\"  Used   : {' '.join(used)}\")\n",
    "    print(f\"  Results:\")\n",
    "    for w, s in res:\n",
    "        print(f\"    {w:<15} {s:.4f}  {'|'*int(s*25)}\")\n",
    "\n",
    "print(\"\\nNote: With a larger dataset (millions of words),\")\n",
    "print(\"king - man + woman = queen would work precisely.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 12 — PCA Visualization (Trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "word_groups = {\n",
    "    \"Verbs\"     : [\"come\",\"go\",\"see\",\"know\",\"think\",\"make\",\"give\",\"take\"],\n",
    "    \"Adjectives\": [\"good\",\"great\",\"old\",\"new\",\"little\",\"own\",\"right\",\"long\"],\n",
    "    \"Nouns\"     : [\"man\",\"men\",\"time\",\"work\",\"hand\",\"eye\",\"room\",\"door\"],\n",
    "    \"Function\"  : [\"the\",\"of\",\"and\",\"to\",\"in\",\"was\",\"he\",\"it\"],\n",
    "}\n",
    "colors = [\"#e74c3c\",\"#2ecc71\",\"#3498db\",\"#f39c12\"]\n",
    "all_w, all_v, all_c = [], [], []\n",
    "\n",
    "for (grp, wlist), col in zip(word_groups.items(), colors):\n",
    "    for w in wlist:\n",
    "        v = get_vec(w)\n",
    "        if v is not None:\n",
    "            all_w.append(w); all_v.append(v.numpy()); all_c.append(col)\n",
    "\n",
    "pca    = PCA(n_components=2)\n",
    "vecs2d = pca.fit_transform(np.array(all_v))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "ax.set_facecolor(\"#f8f9fa\")\n",
    "for w, coord, col in zip(all_w, vecs2d, all_c):\n",
    "    ax.scatter(coord[0], coord[1], c=col, s=120, zorder=3, alpha=0.85)\n",
    "    ax.annotate(w, coord, textcoords=\"offset points\", xytext=(6,4),\n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "patches = [mpatches.Patch(color=c, label=g) for g,c in zip(word_groups.keys(), colors)]\n",
    "ax.legend(handles=patches, fontsize=10, loc='upper right')\n",
    "ax.set_title(\"Word Embedding Space — PCA (Trained CBOW)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pca_trained.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: pca_trained.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 13 — t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne      = TSNE(n_components=2, perplexity=min(15, len(all_w)-1), random_state=42, n_iter=1000)\n",
    "vecs_tsne = tsne.fit_transform(np.array(all_v))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 8))\n",
    "ax.set_facecolor(\"#f8f9fa\")\n",
    "for w, coord, col in zip(all_w, vecs_tsne, all_c):\n",
    "    ax.scatter(coord[0], coord[1], c=col, s=130, zorder=3, alpha=0.85)\n",
    "    ax.annotate(w, coord, textcoords=\"offset points\", xytext=(6,4),\n",
    "                fontsize=9, fontweight='bold')\n",
    "\n",
    "ax.legend(handles=patches, fontsize=10, loc='upper right')\n",
    "ax.set_title(\"Word Embedding Space — t-SNE (Trained CBOW)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"t-SNE Dim 1\")\n",
    "ax.set_ylabel(\"t-SNE Dim 2\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"tsne_trained.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: tsne_trained.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 14 — Cosine Similarity Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity as cos_mat\n",
    "\n",
    "hmap_words = [w for w in [\"good\",\"great\",\"old\",\"new\",\"man\",\"men\",\"come\",\"go\",\"see\",\"know\"] if w in w2i]\n",
    "vecs       = np.array([get_vec(w).numpy() for w in hmap_words])\n",
    "sim_m      = cos_mat(vecs)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 7))\n",
    "im = ax.imshow(sim_m, cmap=\"RdYlGn\", vmin=-1, vmax=1)\n",
    "ax.set_xticks(range(len(hmap_words)))\n",
    "ax.set_yticks(range(len(hmap_words)))\n",
    "ax.set_xticklabels(hmap_words, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(hmap_words, fontsize=10)\n",
    "for i in range(len(hmap_words)):\n",
    "    for j in range(len(hmap_words)):\n",
    "        v = sim_m[i,j]\n",
    "        ax.text(j, i, f\"{v:.2f}\", ha='center', va='center',\n",
    "                fontsize=8, fontweight='bold',\n",
    "                color='white' if abs(v) > 0.5 else 'black')\n",
    "plt.colorbar(im, ax=ax, label=\"Cosine Similarity\")\n",
    "ax.set_title(\"Cosine Similarity Heatmap\", fontsize=13, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"cosine_heatmap.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: cosine_heatmap.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 15 — Self-Attention Mechanism (From Scratch)\n",
    "> Attention lets each token look at every other token and decide which ones are relevant.\n",
    "> Formula: Attention(Q, K, V) = softmax(QKᵀ / √d_k) × V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence    = \"The cat sat on the mat\"\n",
    "words_list  = sentence.lower().split()\n",
    "embed_dim   = 8\n",
    "seq_len     = len(words_list)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "x = torch.randn(seq_len, embed_dim)   # (6, 8)\n",
    "\n",
    "# Q = K = V = x  (simplified self-attention)\n",
    "Q = x;  K = x;  V = x\n",
    "\n",
    "# Step 1: Attention scores\n",
    "scores = torch.matmul(Q, K.T) / (embed_dim ** 0.5)   # (6, 6)\n",
    "\n",
    "# Step 2: Softmax\n",
    "attn_weights = F.softmax(scores, dim=-1)              # (6, 6)\n",
    "\n",
    "# Step 3: Context = weighted sum of V\n",
    "context = torch.matmul(attn_weights, V)               # (6, 8)\n",
    "\n",
    "print(f\"Words       : {words_list}\")\n",
    "print(f\"Input  shape: {x.shape}            (seq=6, dim=8)\")\n",
    "print(f\"Scores shape: {scores.shape}          (seq x seq)\")\n",
    "print(f\"Weights shape:{attn_weights.shape}   (each row sums to 1)\")\n",
    "print(f\"Context shape:{context.shape}        (same as input)\")\n",
    "print(f\"\\nRow sums (should all be 1.0): {attn_weights.sum(dim=-1).detach().numpy().round(3)}\")\n",
    "print(\"\\nAttention Weights (rounded):\")\n",
    "for i, w in enumerate(words_list):\n",
    "    row = attn_weights[i].detach().numpy()\n",
    "    row_str = \"  \".join([f\"{v:.2f}\" for v in row])\n",
    "    print(f\"  {w:<6}  [{row_str}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 16 — Attention Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Full heatmap\n",
    "ax = axes[0]\n",
    "im = ax.imshow(attn_weights.detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(seq_len)); ax.set_yticks(range(seq_len))\n",
    "ax.set_xticklabels(words_list, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(words_list, fontsize=10)\n",
    "for i in range(seq_len):\n",
    "    for j in range(seq_len):\n",
    "        v = attn_weights[i,j].item()\n",
    "        ax.text(j,i,f\"{v:.2f}\",ha='center',va='center',fontsize=8,\n",
    "                color='white' if v>0.4 else 'black')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Self-Attention Weights\", fontsize=11, fontweight='bold')\n",
    "ax.set_xlabel(\"Keys\"); ax.set_ylabel(\"Queries\")\n",
    "\n",
    "# Plot 2: Bar chart for one word\n",
    "ax2   = axes[1]\n",
    "idx_w = 1   # 'cat'\n",
    "vals  = attn_weights[idx_w].detach().numpy()\n",
    "bars  = ax2.bar(words_list, vals,\n",
    "               color=['#e74c3c' if i==idx_w else '#3498db' for i in range(seq_len)], alpha=0.85)\n",
    "for b in bars:\n",
    "    ax2.annotate(f\"{b.get_height():.3f}\",\n",
    "                 xy=(b.get_x()+b.get_width()/2, b.get_height()),\n",
    "                 xytext=(0,3), textcoords='offset points', ha='center', fontsize=9)\n",
    "ax2.set_title(f\"Attention from '{words_list[idx_w]}' to all tokens\",\n",
    "              fontsize=11, fontweight='bold')\n",
    "ax2.set_ylabel(\"Weight\"); ax2.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"attention_weights.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: attention_weights.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 17 — Masked (Causal) Attention — GPT Style\n",
    "> GPT uses causal masking so token[i] can only attend to tokens[0..i], never future ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask          = torch.triu(torch.ones(seq_len, seq_len), diagonal=1) * float('-inf')\n",
    "masked_scores = scores + mask\n",
    "masked_w      = F.softmax(masked_scores, dim=-1)\n",
    "masked_ctx    = torch.matmul(masked_w, V)\n",
    "\n",
    "print(\"Causal Mask (True = masked/blocked):\")\n",
    "print(torch.triu(torch.ones(seq_len, seq_len, dtype=torch.bool), diagonal=1).numpy())\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "for ax, w, title in zip(axes,\n",
    "    [attn_weights, masked_w],\n",
    "    [\"Full Self-Attention\", \"Causal Masked Attention (GPT)\"]):\n",
    "    im = ax.imshow(w.detach().numpy(), cmap='Blues', vmin=0, vmax=1)\n",
    "    ax.set_xticks(range(seq_len)); ax.set_yticks(range(seq_len))\n",
    "    ax.set_xticklabels(words_list, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_yticklabels(words_list, fontsize=9)\n",
    "    for i in range(seq_len):\n",
    "        for j in range(seq_len):\n",
    "            v = w[i,j].item()\n",
    "            ax.text(j,i,f\"{v:.2f}\",ha='center',va='center',fontsize=7,\n",
    "                    color='white' if v>0.4 else 'black')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "    ax.set_title(title, fontsize=10, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"Full vs Causal Masked Attention\", fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"masked_attention.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: masked_attention.png\")\n",
    "print(\"\\nIn causal attention: upper triangle = 0 (future tokens are invisible)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 18 — Random vs Trained Embedding Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_emb = torch.nn.Embedding(V, EMBED_DIM).weight.detach()\n",
    "\n",
    "def cmp_sim(emb, w1, w2):\n",
    "    if w1 not in w2i or w2 not in w2i: return 0\n",
    "    v1 = emb[w2i[w1]].unsqueeze(0)\n",
    "    v2 = emb[w2i[w2]].unsqueeze(0)\n",
    "    return F.cosine_similarity(v1, v2).item()\n",
    "\n",
    "pairs  = [(\"good\",\"great\"),(\"man\",\"men\"),(\"come\",\"go\"),(\"see\",\"know\"),(\"good\",\"door\"),(\"time\",\"eye\")]\n",
    "r_sims = [cmp_sim(random_emb,   w1, w2) for w1, w2 in pairs]\n",
    "t_sims = [cmp_sim(trained_embeddings, w1, w2) for w1, w2 in pairs]\n",
    "lbls   = [f\"{w1}↔{w2}\" for w1, w2 in pairs]\n",
    "\n",
    "x, w = np.arange(len(lbls)), 0.35\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "b1 = ax.bar(x-w/2, r_sims, w, label='Random',  color='#e74c3c', alpha=0.8)\n",
    "b2 = ax.bar(x+w/2, t_sims, w, label='Trained', color='#2ecc71', alpha=0.8)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(lbls, rotation=15, ha='right', fontsize=9)\n",
    "ax.set_ylabel(\"Cosine Similarity\")\n",
    "ax.axhline(0, color='black', linewidth=0.8, linestyle='--')\n",
    "ax.set_title(\"Random vs Trained Embeddings: Cosine Similarity\", fontsize=13, fontweight='bold')\n",
    "ax.legend(fontsize=10); ax.grid(axis='y', alpha=0.3)\n",
    "for bars in [b1, b2]:\n",
    "    for b in bars:\n",
    "        ax.annotate(f\"{b.get_height():.2f}\",\n",
    "                    xy=(b.get_x()+b.get_width()/2, b.get_height()),\n",
    "                    xytext=(0,3), textcoords='offset points', ha='center', fontsize=7)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"random_vs_trained.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: random_vs_trained.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 19 — Full Pipeline Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"       ENGLISH TOKENIZER & VECTOR EMBEDDINGS SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "s   = \"Hello, I am building an LLM from scratch.\"\n",
    "ids = tiktoken.get_encoding(\"gpt2\").encode(s)\n",
    "emb = torch.nn.Embedding(50257, 256)\n",
    "pos = torch.nn.Embedding(len(ids), 256)\n",
    "out = emb(torch.tensor(ids)) + pos(torch.arange(len(ids)))\n",
    "\n",
    "print(f\"\\nInput        : {s}\")\n",
    "print(f\"Token IDs    : {ids}\")\n",
    "print(f\"Num tokens   : {len(ids)}\")\n",
    "print(f\"Embed shape  : {out.shape}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*65)\n",
    "concepts = [\n",
    "    (\"Regex Tokenizer\",   \"Splits text on whitespace + punctuation\"),\n",
    "    (\"BPE Tokenizer\",     \"GPT-2 via tiktoken — 50,257 vocab\"),\n",
    "    (\"Sliding Window\",    \"DataLoader for next-token prediction\"),\n",
    "    (\"Token Embedding\",   \"nn.Embedding: ID -> dense vector\"),\n",
    "    (\"Pos Embedding\",     \"Absolute position added to token vec\"),\n",
    "    (\"Word2Vec CBOW\",     \"Train embeddings from context words\"),\n",
    "    (\"Cosine Similarity\", \"Angle between vectors = semantic sim\"),\n",
    "    (\"Embed Arithmetic\",  \"king - man + woman = queen\"),\n",
    "    (\"Self-Attention\",    \"softmax(QKT/sqrt(d)) x V\"),\n",
    "    (\"Causal Mask\",       \"GPT: no peeking at future tokens\"),\n",
    "    (\"PCA / t-SNE\",       \"2D projection of embedding space\"),\n",
    "]\n",
    "for name, desc in concepts:\n",
    "    print(f\"  [OK] {name:<22} {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*65)\n",
    "print(\"SAVED PLOTS\")\n",
    "saved = [\"training_loss.png\",\"pca_trained.png\",\"tsne_trained.png\",\n",
    "         \"cosine_heatmap.png\",\"attention_weights.png\",\n",
    "         \"masked_attention.png\",\"random_vs_trained.png\"]\n",
    "for f in saved:\n",
    "    status = \"OK\" if os.path.exists(f) else \"MISSING\"\n",
    "    print(f\"  [{status}] {f}\")\n",
    "print(\"=\" * 65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Hindi Tokenizer & Vector Embeddings\n",
    "### HuggingFace Dataset · Custom Tokenizer · Word2Vec · Attention · English vs Hindi\n",
    "---\n",
    "**Topics Covered:**\n",
    "1. Load Hindi dataset from HuggingFace (auto-download)\n",
    "2. GPT-2 tiktoken on Hindi — shows inefficiency\n",
    "3. Custom Hindi Word Tokenizer\n",
    "4. Hindi DataLoader (sliding window)\n",
    "5. Hindi Token + Positional Embeddings\n",
    "6. Word2Vec CBOW Training on Hindi\n",
    "7. Training Loss Curve\n",
    "8. Cosine Similarity for Hindi words\n",
    "9. Embedding Arithmetic on Hindi words\n",
    "10. PCA Visualization of Hindi Embeddings\n",
    "11. t-SNE Visualization\n",
    "12. Attention Mechanism on Hindi\n",
    "13. English vs Hindi Token Count Comparison\n",
    "14. Vocabulary Size Comparison\n",
    "15. Full Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 1 — Install & Version Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to install\n",
    "# !pip install torch tiktoken datasets matplotlib scikit-learn requests\n",
    "\n",
    "import sys\n",
    "from importlib.metadata import version\n",
    "\n",
    "print(\"Python   :\", sys.version)\n",
    "print(\"torch    :\", version(\"torch\"))\n",
    "print(\"tiktoken :\", version(\"tiktoken\"))\n",
    "try:\n",
    "    print(\"datasets :\", version(\"datasets\"))\n",
    "except:\n",
    "    print(\"datasets : NOT INSTALLED — run: pip install datasets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 2 — Load Hindi Dataset from HuggingFace\n",
    "> Uses `cfilt/iitb-english-hindi` — a large parallel corpus. Streaming mode so no full download needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "\n",
    "CACHE_FILE = \"hindi_dataset.txt\"\n",
    "\n",
    "if os.path.exists(CACHE_FILE):\n",
    "    print(\"Loading from cached file...\")\n",
    "    with open(CACHE_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        hindi_raw_text = f.read()\n",
    "    print(\"Loaded from cache.\")\n",
    "else:\n",
    "    print(\"Downloading Hindi dataset from HuggingFace...\")\n",
    "    print(\"(First run only — uses streaming, no large download)\")\n",
    "\n",
    "    hindi_raw_text = \"\"\n",
    "    loaded_from    = \"\"\n",
    "\n",
    "    try:\n",
    "        ds = load_dataset(\"cfilt/iitb-english-hindi\", split=\"train\", streaming=True)\n",
    "        samples = []\n",
    "        for i, row in enumerate(ds):\n",
    "            samples.append(row[\"translation\"][\"hi\"])\n",
    "            if i >= 1999: break\n",
    "        hindi_raw_text = \"\\n\".join(samples)\n",
    "        loaded_from    = \"cfilt/iitb-english-hindi (HuggingFace)\"\n",
    "    except Exception as e:\n",
    "        print(f\"Primary source failed: {e}\")\n",
    "        try:\n",
    "            ds = load_dataset(\"Helsinki-NLP/opus-100\", \"en-hi\", split=\"train\", streaming=True)\n",
    "            samples = []\n",
    "            for i, row in enumerate(ds):\n",
    "                if \"hi\" in row[\"translation\"]:\n",
    "                    samples.append(row[\"translation\"][\"hi\"])\n",
    "                if i >= 1999: break\n",
    "            hindi_raw_text = \"\\n\".join(samples)\n",
    "            loaded_from    = \"Helsinki-NLP/opus-100 (HuggingFace)\"\n",
    "        except Exception as e2:\n",
    "            print(f\"Fallback also failed: {e2}\")\n",
    "            print(\"Using built-in Hindi sample text.\")\n",
    "            hindi_raw_text = \"\"\"\n",
    "भारत एक महान देश है। यहाँ अनेक भाषाएँ बोली जाती हैं।\n",
    "हिंदी भारत की राजभाषा है। यह एक सुंदर और समृद्ध भाषा है।\n",
    "दिल्ली भारत की राजधानी है। मुंबई एक बड़ा शहर है।\n",
    "भारतीय संस्कृति बहुत प्राचीन है। यहाँ के लोग मिलनसार हैं।\n",
    "शिक्षा और ज्ञान का बहुत महत्व है। विज्ञान से देश आगे बढ़ता है।\n",
    "भारत में अनेक त्योहार मनाए जाते हैं जैसे दीवाली होली और ईद।\n",
    "हमारा देश विविधता में एकता का प्रतीक है।\n",
    "भारतीय साहित्य संगीत और कला विश्व प्रसिद्ध है।\n",
    "गंगा और यमुना भारत की प्रमुख नदियाँ हैं।\n",
    "हिमालय पर्वत भारत के उत्तर में स्थित है।\n",
    "युवा पीढ़ी देश के भविष्य का निर्माण कर रही है।\n",
    "अहिंसा और सत्य भारतीय संस्कृति के मूल मूल्य हैं।\n",
    "भारत की अर्थव्यवस्था तेजी से विकास कर रही है।\n",
    "यहाँ के किसान देश की रीढ़ हैं।\n",
    "तकनीक और नवाचार से भारत आगे बढ़ रहा है।\n",
    "स्वास्थ्य सेवाएँ देश के हर कोने तक पहुँच रही हैं।\n",
    "महिलाएँ हर क्षेत्र में आगे बढ़ रही हैं।\n",
    "पर्यावरण की रक्षा करना हम सबकी जिम्मेदारी है।\n",
    "खेल और योग भारत की पहचान हैं।\n",
    "\"\"\" * 10   # repeat to get more data\n",
    "            loaded_from = \"Built-in sample text\"\n",
    "\n",
    "    with open(CACHE_FILE, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(hindi_raw_text)\n",
    "    print(f\"Loaded from: {loaded_from}\")\n",
    "    print(f\"Saved to  : {CACHE_FILE}\")\n",
    "\n",
    "print(f\"\\nTotal characters : {len(hindi_raw_text):,}\")\n",
    "print(f\"Total lines      : {hindi_raw_text.count(chr(10)):,}\")\n",
    "print(f\"\\nFirst 300 chars:\\n{hindi_raw_text[:300]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 3 — GPT-2 Tokenizer on Hindi (shows byte-level splitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "gpt2_tok = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "hi_sample = \"भारत एक महान देश है।\"\n",
    "en_sample = \"India is a great country.\"\n",
    "\n",
    "hi_ids = gpt2_tok.encode(hi_sample)\n",
    "en_ids = gpt2_tok.encode(en_sample)\n",
    "\n",
    "print(\"=== GPT-2 on Hindi vs English ===\")\n",
    "print(f\"\\nHindi : '{hi_sample}'\")\n",
    "print(f\"  IDs  : {hi_ids}\")\n",
    "print(f\"  Count: {len(hi_ids)} tokens\")\n",
    "\n",
    "print(f\"\\nEnglish: '{en_sample}'\")\n",
    "print(f\"  IDs  : {en_ids}\")\n",
    "print(f\"  Count: {len(en_ids)} tokens\")\n",
    "\n",
    "print(f\"\\nHindi needs {len(hi_ids)/len(en_ids):.1f}x more GPT-2 tokens for the same meaning!\")\n",
    "\n",
    "print(\"\\n--- Hindi token-by-token (GPT-2 splits Hindi into raw bytes) ---\")\n",
    "for tid in hi_ids:\n",
    "    try:    text = gpt2_tok.decode([tid])\n",
    "    except: text = \"[byte]\"\n",
    "    print(f\"  ID {tid:6d}  ->  {repr(text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 4 — Custom Hindi Word Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class HindiTokenizer:\n",
    "    \"\"\"Word-level Hindi tokenizer. Handles spaces and Hindi punctuation.\"\"\"\n",
    "    def __init__(self, text):\n",
    "        tokens  = self._split(text)\n",
    "        unique  = sorted(set(tokens))\n",
    "        unique  = [\"<|unk|>\", \"<|endoftext|>\"] + unique\n",
    "        self.str_to_int = {t: i for i, t in enumerate(unique)}\n",
    "        self.int_to_str = {i: t for t, i in self.str_to_int.items()}\n",
    "\n",
    "    def _split(self, text):\n",
    "        tokens = re.split(r'(\\s|।|,|!|\\?|;|:|\\.)', text)\n",
    "        return [t.strip() for t in tokens if t.strip()]\n",
    "\n",
    "    def encode(self, text):\n",
    "        unk = self.str_to_int[\"<|unk|>\"]\n",
    "        return [self.str_to_int.get(t, unk) for t in self._split(text)]\n",
    "\n",
    "    def decode(self, ids):\n",
    "        return \" \".join([self.int_to_str.get(i, \"<|unk|>\") for i in ids])\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self): return len(self.str_to_int)\n",
    "\n",
    "\n",
    "hindi_tokenizer = HindiTokenizer(hindi_raw_text)\n",
    "print(f\"Hindi vocabulary size: {hindi_tokenizer.vocab_size:,}\")\n",
    "\n",
    "# Test\n",
    "test = \"भारत एक महान देश है।\"\n",
    "enc  = hindi_tokenizer.encode(test)\n",
    "dec  = hindi_tokenizer.decode(enc)\n",
    "print(f\"\\nOriginal : {test}\")\n",
    "print(f\"Token IDs: {enc}\")\n",
    "print(f\"Decoded  : {dec}\")\n",
    "\n",
    "print(\"\\n--- First 20 vocab entries ---\")\n",
    "for t, i in list(hindi_tokenizer.str_to_int.items())[:20]:\n",
    "    print(f\"  {i:4d}  ->  '{t}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 5 — Hindi DataLoader (Sliding Window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HindiDataset(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        self.inputs, self.targets = [], []\n",
    "        ids = tokenizer.encode(text)\n",
    "        print(f\"Total Hindi tokens: {len(ids):,}\")\n",
    "        for i in range(0, len(ids) - max_length, stride):\n",
    "            self.inputs.append(torch.tensor(ids[i:i+max_length]))\n",
    "            self.targets.append(torch.tensor(ids[i+1:i+max_length+1]))\n",
    "    def __len__(self):         return len(self.inputs)\n",
    "    def __getitem__(self, i):  return self.inputs[i], self.targets[i]\n",
    "\n",
    "max_length = 4\n",
    "h_ds     = HindiDataset(hindi_raw_text, hindi_tokenizer, max_length, stride=2)\n",
    "h_loader = DataLoader(h_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "h_inputs, h_targets = next(iter(h_loader))\n",
    "print(\"\\nInput shape:\", h_inputs.shape)\n",
    "print(\"\\nDecoded inputs:\")\n",
    "for row in h_inputs:\n",
    "    print(\" \", hindi_tokenizer.decode(row.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 6 — Hindi Token + Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dim = 64\n",
    "\n",
    "tok_emb_layer = torch.nn.Embedding(hindi_tokenizer.vocab_size, output_dim)\n",
    "pos_emb_layer = torch.nn.Embedding(max_length, output_dim)\n",
    "\n",
    "tok_embs = tok_emb_layer(h_inputs)\n",
    "pos_embs = pos_emb_layer(torch.arange(max_length))\n",
    "final    = tok_embs + pos_embs\n",
    "\n",
    "print(\"=== Hindi Embedding Shapes ===\")\n",
    "print(f\"Vocab size          : {hindi_tokenizer.vocab_size:,}\")\n",
    "print(f\"Embed dim           : {output_dim}\")\n",
    "print(f\"h_inputs shape      : {h_inputs.shape}\")\n",
    "print(f\"tok_embs shape      : {tok_embs.shape}\")\n",
    "print(f\"pos_embs shape      : {pos_embs.shape}\")\n",
    "print(f\"final shape         : {final.shape}\")\n",
    "print(f\"\\nFirst token vector (8 dims): {final[0][0][:8].detach().numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 7 — Word2Vec CBOW Training on Hindi\n",
    "> Same CBOW approach as English — learn Hindi word vectors from context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Build Hindi word list\n",
    "all_hindi_words = re.split(r'\\s+|।', hindi_raw_text)\n",
    "all_hindi_words = [w.strip() for w in all_hindi_words if w.strip() and len(w) > 1]\n",
    "\n",
    "freq        = Counter(all_hindi_words)\n",
    "h_vocab     = [w for w, c in freq.most_common(600)]\n",
    "hw2i        = {w: i for i, w in enumerate(h_vocab)}\n",
    "hi2w        = {i: w for w, i in hw2i.items()}\n",
    "HV          = len(h_vocab)\n",
    "print(f\"Hindi training vocab: {HV}\")\n",
    "\n",
    "# CBOW pairs\n",
    "WINDOW   = 2\n",
    "h_data   = []\n",
    "h_filt   = [w for w in all_hindi_words if w in hw2i]\n",
    "\n",
    "for i in range(WINDOW, len(h_filt) - WINDOW):\n",
    "    ctx = ([h_filt[i-j] for j in range(WINDOW, 0, -1)] +\n",
    "           [h_filt[i+j] for j in range(1, WINDOW+1)])\n",
    "    ctr = h_filt[i]\n",
    "    if all(w in hw2i for w in ctx) and ctr in hw2i:\n",
    "        h_data.append(([hw2i[w] for w in ctx], hw2i[ctr]))\n",
    "\n",
    "random.shuffle(h_data)\n",
    "print(f\"Training samples   : {len(h_data):,}\")\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.linear    = nn.Linear(embed_dim, vocab_size)\n",
    "    def forward(self, ctx):\n",
    "        return self.linear(self.embedding(ctx).mean(dim=1))\n",
    "\n",
    "H_EMBED   = 64\n",
    "H_EPOCHS  = 8\n",
    "H_BATCH   = 256\n",
    "\n",
    "h_model    = CBOWModel(HV, H_EMBED)\n",
    "h_crit     = nn.CrossEntropyLoss()\n",
    "h_opt      = optim.Adam(h_model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"\\nTraining Hindi Word2Vec CBOW...\")\n",
    "h_loss_hist = []\n",
    "\n",
    "for epoch in range(H_EPOCHS):\n",
    "    random.shuffle(h_data)\n",
    "    total, steps = 0, 0\n",
    "    for start in range(0, len(h_data), H_BATCH):\n",
    "        batch = h_data[start:start+H_BATCH]\n",
    "        ctx_t = torch.tensor([d[0] for d in batch])\n",
    "        ctr_t = torch.tensor([d[1] for d in batch])\n",
    "        h_opt.zero_grad()\n",
    "        loss = h_crit(h_model(ctx_t), ctr_t)\n",
    "        loss.backward()\n",
    "        h_opt.step()\n",
    "        total += loss.item(); steps += 1\n",
    "    avg = total / steps\n",
    "    h_loss_hist.append(avg)\n",
    "    print(f\"  Epoch {epoch+1}/{H_EPOCHS}  Loss: {avg:.4f}\")\n",
    "\n",
    "h_trained_emb = h_model.embedding.weight.detach()\n",
    "print(\"\\nHindi training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 8 — Hindi Training Loss Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(range(1, H_EPOCHS+1), h_loss_hist, 's-',\n",
    "        color='#e74c3c', linewidth=2.5, markersize=8)\n",
    "ax.fill_between(range(1, H_EPOCHS+1), h_loss_hist, alpha=0.15, color='#e74c3c')\n",
    "ax.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax.set_ylabel(\"Cross-Entropy Loss\", fontsize=12)\n",
    "ax.set_title(\"Hindi Word2Vec CBOW Training Loss\", fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hindi_training_loss.png\", dpi=150)\n",
    "plt.show()\n",
    "print(\"Saved: hindi_training_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 9 — Cosine Similarity for Hindi Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_h_vec(word):\n",
    "    if word not in hw2i: return None\n",
    "    return h_trained_emb[hw2i[word]]\n",
    "\n",
    "def h_cosine_sim(w1, w2):\n",
    "    v1, v2 = get_h_vec(w1), get_h_vec(w2)\n",
    "    if v1 is None or v2 is None: return None\n",
    "    return F.cosine_similarity(v1.unsqueeze(0), v2.unsqueeze(0)).item()\n",
    "\n",
    "def h_top_similar(word, topn=5):\n",
    "    if word not in hw2i: return []\n",
    "    vec  = h_trained_emb[hw2i[word]].unsqueeze(0)\n",
    "    sims = F.cosine_similarity(vec, h_trained_emb).detach().numpy()\n",
    "    top  = np.argsort(sims)[::-1][1:topn+1]\n",
    "    return [(hi2w[i], sims[i]) for i in top]\n",
    "\n",
    "print(\"=\" * 55)\n",
    "print(\"GROUP 1 — Related Hindi words\")\n",
    "print(\"=\" * 55)\n",
    "related = [(\"भारत\",\"देश\"),(\"हिंदी\",\"भाषा\"),(\"शिक्षा\",\"ज्ञान\"),\n",
    "           (\"दिल्ली\",\"मुंबई\"),(\"गंगा\",\"यमुना\")]\n",
    "for w1, w2 in related:\n",
    "    s = h_cosine_sim(w1, w2)\n",
    "    if s: print(f\"  {w1:<14} <-> {w2:<14}  {s:+.4f}\")\n",
    "    else: print(f\"  {w1:<14} <-> {w2:<14}  (not in vocab)\")\n",
    "\n",
    "print(\"\\nGROUP 2 — Unrelated Hindi words\")\n",
    "print(\"-\" * 55)\n",
    "unrelated = [(\"भारत\",\"शिक्षा\"),(\"हिमालय\",\"भाषा\"),(\"नदी\",\"युवा\")]\n",
    "for w1, w2 in unrelated:\n",
    "    s = h_cosine_sim(w1, w2)\n",
    "    if s: print(f\"  {w1:<14} <-> {w2:<14}  {s:+.4f}\")\n",
    "    else: print(f\"  {w1:<14} <-> {w2:<14}  (not in vocab)\")\n",
    "\n",
    "print(\"\\n--- Top 5 similar Hindi words ---\")\n",
    "for q in [\"भारत\", \"शिक्षा\", \"भाषा\"]:\n",
    "    sim = h_top_similar(q)\n",
    "    if sim:\n",
    "        print(f\"  '{q}' -> {', '.join([f\\\"{w}({s:.2f})\\\" for w,s in sim])}\")\n",
    "    else:\n",
    "        print(f\"  '{q}' -> not in vocab\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 10 — Embedding Arithmetic on Hindi Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h_embedding_arithmetic(pos_words, neg_words, topn=5):\n",
    "    result = torch.zeros(H_EMBED)\n",
    "    used, missing = [], []\n",
    "    for w in pos_words:\n",
    "        v = get_h_vec(w)\n",
    "        if v is not None: result += v; used.append(f\"+{w}\")\n",
    "        else: missing.append(w)\n",
    "    for w in neg_words:\n",
    "        v = get_h_vec(w)\n",
    "        if v is not None: result -= v; used.append(f\"-{w}\")\n",
    "        else: missing.append(w)\n",
    "    result = F.normalize(result.unsqueeze(0), dim=1).squeeze()\n",
    "    sims   = F.cosine_similarity(result.unsqueeze(0), h_trained_emb).detach().numpy()\n",
    "    excl   = set(pos_words + neg_words)\n",
    "    ranked = np.argsort(sims)[::-1]\n",
    "    res    = []\n",
    "    for idx in ranked:\n",
    "        w = hi2w[idx]\n",
    "        if w not in excl: res.append((w, sims[idx]))\n",
    "        if len(res) == topn: break\n",
    "    return used, missing, res\n",
    "\n",
    "# Hindi arithmetic queries\n",
    "h_queries = [\n",
    "    ([\"भारत\", \"भाषा\"], [\"देश\"],   \"भारत + भाषा - देश\"),\n",
    "    ([\"शिक्षा\", \"ज्ञान\"], [\"युवा\"], \"शिक्षा + ज्ञान - युवा\"),\n",
    "    ([\"दिल्ली\"], [\"मुंबई\"],         \"दिल्ली - मुंबई\"),\n",
    "    ([\"गंगा\", \"नदी\"], [\"यमुना\"],    \"गंगा + नदी - यमुना\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"HINDI EMBEDDING ARITHMETIC\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for pos, neg, label in h_queries:\n",
    "    used, miss, res = h_embedding_arithmetic(pos, neg)\n",
    "    print(f\"\\n  Query  : {label}\")\n",
    "    if miss: print(f\"  Missing: {miss}\")\n",
    "    if not used: print(\"  (all words missing from vocab)\"); continue\n",
    "    print(f\"  Used   : {' '.join(used)}\")\n",
    "    print(f\"  Results:\")\n",
    "    for w, s in res:\n",
    "        print(f\"    {w:<18} {s:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 11 — PCA Visualization of Hindi Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "h_word_groups = {\n",
    "    \"देश/Place\" : [\"भारत\",\"दिल्ली\",\"मुंबई\",\"देश\",\"शहर\",\"गाँव\"],\n",
    "    \"भाषा/Language\": [\"हिंदी\",\"भाषा\",\"शब्द\",\"साहित्य\",\"संस्कृति\"],\n",
    "    \"शिक्षा/Knowledge\": [\"शिक्षा\",\"ज्ञान\",\"विज्ञान\",\"पुस्तक\",\"विद्यालय\"],\n",
    "    \"प्रकृति/Nature\": [\"हिमालय\",\"गंगा\",\"नदी\",\"पर्वत\",\"यमुना\"],\n",
    "}\n",
    "h_colors = [\"#e74c3c\",\"#2ecc71\",\"#3498db\",\"#f39c12\"]\n",
    "\n",
    "hw, hv, hc = [], [], []\n",
    "for (grp, wlist), col in zip(h_word_groups.items(), h_colors):\n",
    "    for w in wlist:\n",
    "        v = get_h_vec(w)\n",
    "        if v is not None:\n",
    "            hw.append(w); hv.append(v.numpy()); hc.append(col)\n",
    "\n",
    "if len(hv) >= 4:\n",
    "    pca    = PCA(n_components=2)\n",
    "    h2d    = pca.fit_transform(np.array(hv))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.set_facecolor(\"#f8f9fa\")\n",
    "    for w, coord, col in zip(hw, h2d, hc):\n",
    "        ax.scatter(coord[0], coord[1], c=col, s=140, zorder=3, alpha=0.85)\n",
    "        ax.annotate(w, coord, textcoords=\"offset points\",\n",
    "                    xytext=(6,4), fontsize=10, fontweight='bold')\n",
    "\n",
    "    h_patches = [mpatches.Patch(color=c, label=g)\n",
    "                 for g, c in zip(h_word_groups.keys(), h_colors)]\n",
    "    ax.legend(handles=h_patches, fontsize=9, loc='upper right')\n",
    "    ax.set_title(\"Hindi Word Embedding Space — PCA\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "    ax.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"hindi_pca.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: hindi_pca.png\")\n",
    "else:\n",
    "    print(\"Not enough words in vocab for PCA. Use a larger dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 12 — t-SNE Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "if len(hv) >= 4:\n",
    "    perp     = min(10, len(hw)-1)\n",
    "    tsne     = TSNE(n_components=2, perplexity=perp, random_state=42, n_iter=1000)\n",
    "    h_tsne   = tsne.fit_transform(np.array(hv))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    ax.set_facecolor(\"#f8f9fa\")\n",
    "    for w, coord, col in zip(hw, h_tsne, hc):\n",
    "        ax.scatter(coord[0], coord[1], c=col, s=140, zorder=3, alpha=0.85)\n",
    "        ax.annotate(w, coord, textcoords=\"offset points\",\n",
    "                    xytext=(6,4), fontsize=10, fontweight='bold')\n",
    "\n",
    "    ax.legend(handles=h_patches, fontsize=9, loc='upper right')\n",
    "    ax.set_title(\"Hindi Word Embedding Space — t-SNE\", fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel(\"Dim 1\"); ax.set_ylabel(\"Dim 2\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"hindi_tsne.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    print(\"Saved: hindi_tsne.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 13 — Attention Mechanism on Hindi Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_sentence  = \"भारत एक महान देश है\"\n",
    "h_words_seq = h_sentence.split()\n",
    "h_seq_len   = len(h_words_seq)\n",
    "h_edim      = 8\n",
    "\n",
    "torch.manual_seed(42)\n",
    "hx = torch.randn(h_seq_len, h_edim)\n",
    "\n",
    "# Self-attention\n",
    "h_scores  = torch.matmul(hx, hx.T) / (h_edim ** 0.5)\n",
    "h_weights = F.softmax(h_scores, dim=-1)\n",
    "h_context = torch.matmul(h_weights, hx)\n",
    "\n",
    "print(f\"Hindi sentence  : {h_sentence}\")\n",
    "print(f\"Words           : {h_words_seq}\")\n",
    "print(f\"Input shape     : {hx.shape}\")\n",
    "print(f\"Weights shape   : {h_weights.shape}\")\n",
    "print(f\"Context shape   : {h_context.shape}\")\n",
    "print(\"\\nAttention weights (each row = how much this word attends to others):\")\n",
    "for i, w in enumerate(h_words_seq):\n",
    "    row = h_weights[i].detach().numpy()\n",
    "    print(f\"  {w:<12}  [{' '.join([f'{v:.2f}' for v in row])}]\")\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "ax = axes[0]\n",
    "im = ax.imshow(h_weights.detach().numpy(), cmap='Oranges', vmin=0, vmax=1)\n",
    "ax.set_xticks(range(h_seq_len)); ax.set_yticks(range(h_seq_len))\n",
    "ax.set_xticklabels(h_words_seq, rotation=45, ha='right', fontsize=10)\n",
    "ax.set_yticklabels(h_words_seq, fontsize=10)\n",
    "for i in range(h_seq_len):\n",
    "    for j in range(h_seq_len):\n",
    "        v = h_weights[i,j].item()\n",
    "        ax.text(j,i,f\"{v:.2f}\",ha='center',va='center',fontsize=8,\n",
    "                color='white' if v>0.4 else 'black')\n",
    "plt.colorbar(im, ax=ax)\n",
    "ax.set_title(\"Hindi Self-Attention Weights\", fontsize=11, fontweight='bold')\n",
    "\n",
    "# Causal masked\n",
    "mask     = torch.triu(torch.ones(h_seq_len, h_seq_len), diagonal=1) * float('-inf')\n",
    "h_mw     = F.softmax(h_scores + mask, dim=-1)\n",
    "ax2 = axes[1]\n",
    "im2 = ax2.imshow(h_mw.detach().numpy(), cmap='Oranges', vmin=0, vmax=1)\n",
    "ax2.set_xticks(range(h_seq_len)); ax2.set_yticks(range(h_seq_len))\n",
    "ax2.set_xticklabels(h_words_seq, rotation=45, ha='right', fontsize=10)\n",
    "ax2.set_yticklabels(h_words_seq, fontsize=10)\n",
    "for i in range(h_seq_len):\n",
    "    for j in range(h_seq_len):\n",
    "        v = h_mw[i,j].item()\n",
    "        ax2.text(j,i,f\"{v:.2f}\",ha='center',va='center',fontsize=8,\n",
    "                color='white' if v>0.4 else 'black')\n",
    "plt.colorbar(im2, ax=ax2)\n",
    "ax2.set_title(\"Hindi Causal Masked Attention\", fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.suptitle(\"Hindi Attention: Full vs Causal\", fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"hindi_attention.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: hindi_attention.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 14 — English vs Hindi Token Count Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translation_pairs = [\n",
    "    (\"India is a great country.\",           \"भारत एक महान देश है।\"),\n",
    "    (\"Hindi is the national language.\",     \"हिंदी राजभाषा है।\"),\n",
    "    (\"Education is very important.\",        \"शिक्षा बहुत महत्वपूर्ण है।\"),\n",
    "    (\"Delhi is the capital of India.\",      \"दिल्ली भारत की राजधानी है।\"),\n",
    "    (\"Science advances the nation.\",        \"विज्ञान से देश आगे बढ़ता है।\"),\n",
    "]\n",
    "\n",
    "print(\"=\" * 72)\n",
    "print(f\"{'Sentence pair':<44} {'EN-GPT2':>7} {'HI-GPT2':>8} {'HI-Custom':>10}\")\n",
    "print(\"=\" * 72)\n",
    "\n",
    "en_gpt2_c, hi_gpt2_c, hi_cust_c = [], [], []\n",
    "for eng, hin in translation_pairs:\n",
    "    ec = len(gpt2_tok.encode(eng))\n",
    "    hg = len(gpt2_tok.encode(hin))\n",
    "    hc = len(hindi_tokenizer.encode(hin))\n",
    "    en_gpt2_c.append(ec); hi_gpt2_c.append(hg); hi_cust_c.append(hc)\n",
    "    print(f\"EN: {eng[:40]:<42} {ec:>7}\")\n",
    "    print(f\"HI: {hin[:40]:<42} {hg:>15} {hc:>10}\")\n",
    "    print(\"-\" * 72)\n",
    "\n",
    "# Bar chart\n",
    "lbls = [f\"Pair {i+1}\" for i in range(len(translation_pairs))]\n",
    "x, w = np.arange(len(lbls)), 0.25\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "b1 = ax.bar(x-w,   en_gpt2_c, w, label='English (GPT-2)',  color='#3498db', alpha=0.85)\n",
    "b2 = ax.bar(x,     hi_gpt2_c, w, label='Hindi (GPT-2)',    color='#e74c3c', alpha=0.85)\n",
    "b3 = ax.bar(x+w,   hi_cust_c, w, label='Hindi (Custom)',   color='#2ecc71', alpha=0.85)\n",
    "ax.set_xticks(x); ax.set_xticklabels(lbls)\n",
    "ax.set_ylabel(\"Token Count\"); ax.legend(fontsize=10)\n",
    "ax.set_title(\"Token Counts: English GPT-2 vs Hindi GPT-2 vs Hindi Custom\",\n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "for bars in [b1,b2,b3]:\n",
    "    for b in bars:\n",
    "        ax.annotate(str(int(b.get_height())),\n",
    "                    xy=(b.get_x()+b.get_width()/2, b.get_height()),\n",
    "                    xytext=(0,3), textcoords='offset points', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"en_vs_hi_tokens.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: en_vs_hi_tokens.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 15 — Vocabulary Size Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "en_custom_vocab = 1132\n",
    "if os.path.exists(\"the-verdict.txt\"):\n",
    "    with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        et = f.read()\n",
    "    etoks = re.split(r'([,.:;?_!\"()\\']|--|\\s)', et)\n",
    "    en_custom_vocab = len(set([t.strip() for t in etoks if t.strip()]))\n",
    "\n",
    "vocab_items = {\n",
    "    \"GPT-2\\n(English)\": 50257,\n",
    "    \"English\\nCustom\":  en_custom_vocab,\n",
    "    \"Hindi\\nCustom\":    hindi_tokenizer.vocab_size,\n",
    "    \"Hindi\\nCBOW\":      HV,\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9, 5))\n",
    "bars = ax.bar(vocab_items.keys(), vocab_items.values(),\n",
    "              color=[\"#3498db\",\"#2ecc71\",\"#e74c3c\",\"#f39c12\"], alpha=0.85, width=0.5)\n",
    "for b in bars:\n",
    "    ax.annotate(f\"{int(b.get_height()):,}\",\n",
    "                xy=(b.get_x()+b.get_width()/2, b.get_height()),\n",
    "                xytext=(0,5), textcoords='offset points',\n",
    "                ha='center', fontsize=11, fontweight='bold')\n",
    "ax.set_ylabel(\"Vocabulary Size\")\n",
    "ax.set_title(\"Vocabulary Size Comparison\", fontsize=13, fontweight='bold')\n",
    "ax.set_yscale('log')\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"vocab_comparison.png\", dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Saved: vocab_comparison.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "##  Cell 16 — Full Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 65)\n",
    "print(\"       HINDI TOKENIZER & VECTOR EMBEDDINGS SUMMARY\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "test_hi  = \"भारत एक महान देश है।\"\n",
    "gpt2_ids = gpt2_tok.encode(test_hi)\n",
    "cust_ids = hindi_tokenizer.encode(test_hi)\n",
    "\n",
    "print(f\"\\nHindi Input      : {test_hi}\")\n",
    "print(f\"GPT-2 IDs        : {gpt2_ids}  ({len(gpt2_ids)} tokens)\")\n",
    "print(f\"Custom IDs       : {cust_ids}  ({len(cust_ids)} tokens)\")\n",
    "print(f\"\\nHindi vocab size : {hindi_tokenizer.vocab_size:,}\")\n",
    "print(f\"CBOW vocab size  : {HV}\")\n",
    "print(f\"GPT-2 vocab size : 50,257\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*65)\n",
    "print(\"CONCEPTS COVERED\")\n",
    "print(\"-\"*65)\n",
    "concepts = [\n",
    "    (\"HuggingFace Dataset\", \"Auto-downloaded Hindi text corpus\"),\n",
    "    (\"GPT-2 on Hindi\",      \"Inefficient: splits Hindi into bytes\"),\n",
    "    (\"Custom Tokenizer\",    \"Word-level, designed for Hindi\"),\n",
    "    (\"DataLoader\",          \"Sliding window next-token batches\"),\n",
    "    (\"Token Embedding\",     \"nn.Embedding for Hindi vocab\"),\n",
    "    (\"Pos Embedding\",       \"Absolute position added\"),\n",
    "    (\"Word2Vec CBOW\",       \"Trained Hindi word vectors\"),\n",
    "    (\"Cosine Similarity\",   \"Hindi semantic similarity\"),\n",
    "    (\"Embed Arithmetic\",    \"Hindi vector operations\"),\n",
    "    (\"Attention\",           \"Self + causal mask on Hindi\"),\n",
    "    (\"EN vs HI Compare\",    \"Token count + vocab size charts\"),\n",
    "]\n",
    "for name, desc in concepts:\n",
    "    print(f\"  [OK] {name:<22} {desc}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*65)\n",
    "print(\"SAVED PLOTS\")\n",
    "saved = [\"hindi_training_loss.png\",\"hindi_pca.png\",\"hindi_tsne.png\",\n",
    "         \"hindi_attention.png\",\"en_vs_hi_tokens.png\",\"vocab_comparison.png\"]\n",
    "for f in saved:\n",
    "    status = \"OK\" if os.path.exists(f) else \"MISSING\"\n",
    "    print(f\"  [{status}] {f}\")\n",
    "\n",
    "print(\"\\n\" + \"-\"*65)\n",
    "print(\"KEY INSIGHT\")\n",
    "print(\"-\"*65)\n",
    "print(\"GPT-2 splits Hindi Unicode into raw bytes → many tokens/word\")\n",
    "print(\"Custom word tokenizer keeps whole words → fewer, meaningful tokens\")\n",
    "print(\"For production: train a BPE tokenizer specifically on Hindi text\")\n",
    "print(\"=\" * 65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
